from __future__ import annotations

import logging
import os
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple

from app.config import LLMConfig

logger = logging.getLogger(__name__)


class BaseLLMClient(ABC):
    """Abstract interface for any chat-completion style LLM."""

    @abstractmethod
    def generate(self, messages: List[Dict[str, str]], max_tokens: int = 512) -> str:
        raise NotImplementedError


class DummyLLMClient(BaseLLMClient):
    """
    Fallback LLM used when no real provider is configured.
    It simply stitches together retrieved context and echoes the user intent.
    """

    def generate(self, messages: List[Dict[str, str]], max_tokens: int = 512) -> str:
        user_msg = [m["content"] for m in messages if m["role"] == "user"][-1]
        system_hint = (
            "Note: A production LLM provider is not configured. "
            "This is a placeholder response generated by DummyLLMClient."
        )
        return f"{system_hint}\n\nYou asked: {user_msg}\n\nPlease configure a real LLM (e.g. OpenAI) to get smarter, natural answers."


class OpenAIChatClient(BaseLLMClient):
    """
    Example implementation using OpenAI's Chat Completions API.
    Requires:
      - LLM_PROVIDER=openai
      - LLM_API_KEY=<your key>
      - openai package installed
    """

    def __init__(self, model_name: str, api_key: str):
        try:
            from openai import OpenAI  # type: ignore
        except ImportError as exc:
            raise RuntimeError(
                "openai package is not installed. Run `pip install openai` "
                "or switch LLM_PROVIDER to 'dummy'."
            ) from exc

        self.client = OpenAI(api_key=api_key)
        self.model_name = model_name

    def generate(self, messages: List[Dict[str, str]], max_tokens: int = 512) -> str:
        try:
            completion = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                max_tokens=max_tokens,
            )
            return completion.choices[0].message.content or ""
        except Exception as e:
            logger.error("OpenAIChatClient error: %s", e, exc_info=True)
            return "There was an error contacting the language model. Please try again later."


def get_llm_client(cfg: LLMConfig) -> Tuple[BaseLLMClient, str]:
    """
    Factory that returns a concrete LLM client and a human-readable status string.
    """
    provider = (cfg.provider or "").lower()

    if provider == "openai" and cfg.api_key:
        logger.info("Initializing OpenAIChatClient with model %s", cfg.model_name)
        client = OpenAIChatClient(model_name=cfg.model_name, api_key=cfg.api_key)
        return client, f"OpenAI Â· {cfg.model_name}"
    else:
        logger.warning(
            "No valid LLM provider configured. Falling back to DummyLLMClient. "
            "Set LLM_PROVIDER and LLM_API_KEY to use a real model."
        )
        client = DummyLLMClient()
        return client, "Dummy LLM (for development)"
